{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8855314-497b-4eb6-9c19-ec749e89b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import dotenv_values\n",
    "import concurrent.futures\n",
    "import dashscope\n",
    "import pymysql\n",
    "import json\n",
    "import re\n",
    "# from zhipuai import ZhipuAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb213b-dc14-4df2-96f5-c5b87c446579",
   "metadata": {},
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e03f25-b04a-4d07-9c8b-657ec50e8204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_df(database, command, columns):\n",
    "    USER = '<USER>'\n",
    "    PWD = '<PASSWORD>'\n",
    "    URL = '<URL>'\n",
    "    conn = pymysql.connect(host=URL, port=3306, user=USER, passwd=PWD, db=database, charset='utf8mb4')\n",
    "    \n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(command)\n",
    "        result = cursor.fetchall()\n",
    "        df = pd.DataFrame(result, columns=columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b924ccb-9cd4-4db6-a9e4-f4da85e1b323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 ms, sys: 783 µs, total: 48.4 ms\n",
      "Wall time: 72.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SQL_LOAD = '''\n",
    "SELECT `id`, `content`\n",
    "FROM `tagged_content_by_kwd_dict_3000`\n",
    "'''\n",
    "\n",
    "df_xhs = load_data_to_df('validation', SQL_LOAD, columns=['id', 'content'])\n",
    "            \n",
    "len(df_xhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbf4b5bf-3940-49e8-ac9f-897b64f736b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overlap = set(df_old_samp['id']).intersection(set(df['id']))\n",
    "# len(overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9633010-a7e5-478e-a651-4fec026e4c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2992, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_xhs = df[~df['id'].isin(overlap)]\n",
    "# df_xhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a613e7-39e9-4908-9ea7-98e9bc1b1d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...</td>\n",
       "      <td>一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...</td>\n",
       "      <td>{'食品类别': '未知', '是否为零售食品': '否', '文本主题': '日常生活挑战...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...   \n",
       "\n",
       "                                             content  \\\n",
       "0  一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...   \n",
       "\n",
       "                                              output  \n",
       "0  {'食品类别': '未知', '是否为零售食品': '否', '文本主题': '日常生活挑战...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_xhs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68798b99-5f7b-460c-8189-4ee1235474c5",
   "metadata": {},
   "source": [
    "# 1. Tag using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268ddc22-3991-4828-8c1c-1181453123ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'qwen1.5-14b-chat'\n",
    "NUM_PARALLEL = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56d1049-9001-42ec-b2b5-c0c01ed6851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashscope.api_key = \"<API_KEY>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e6cf31-a5c9-48dd-b72f-2b5090bffa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chat_completion(model, messages, temperature=0.85):\n",
    "    response = dashscope.Generation.call(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        result_format='message', \n",
    "        temperature=temperature\n",
    "    )\n",
    "    try:\n",
    "        return response['output']['choices'][0]['message']['content']\n",
    "    except:\n",
    "        return '{}'\n",
    "\n",
    "\n",
    "def item_process_tag(item, col_name, model_version, prompt):\n",
    "\n",
    "    # item format: (int, pd.Series)\n",
    "    row = item[1]\n",
    "\n",
    "    messages = [{'role': 'system', 'content': prompt}\n",
    "                , {'role': 'user', 'content': f'{row[col_name]}'}]\n",
    "\n",
    "    response = get_chat_completion(model_version, messages)\n",
    "    try: \n",
    "        idx_start, idx_end = response.find('{'), response.find('}')\n",
    "\n",
    "        row['output'] = response[idx_start:(idx_end+1)].replace('\\n', '')\n",
    "    except:\n",
    "        row['output'] = response\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8e1246-528a-448b-ae69-b8aad3b54ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = '''\n",
    "\"你是一位国际食品零售企业的数字营销资深分析师，擅长消费者洞察。\n",
    "# 牢记你的任务和分析目标：\n",
    "* 你的任务：根据你的理解，甄别平台帖子内容，对文本主题进行标注\n",
    "\n",
    "# 取值范围：\n",
    "* 属性值列表: \"美食分享与推荐\", \"健康与饮食\", \"健康与养生\", \"运动健身\"\n",
    "            , \"户外活动\", \"广告与推广\", \"食物评测\", \"日常记录\", \"烹饪与食谱\"\n",
    "            , \"生活感悟\", \"节日庆祝\", \"体重管理\", \"家庭日常\", \"社交互动\"\n",
    "            , \"旅行\", \"怀旧与回忆\", \"学习与教育\", \"校园生活\"\n",
    "            , \"孕期饮食与健康\", \"儿童饮食与健康\", \"其他\"\n",
    "\n",
    "# 输出格式\n",
    "* 输出JSON格式的回复，包含两个子字段，\"标签值\", \"原文依据\"。\"标签值\"包含一个标签值。\"原文依据\"包含一个原文关键字词的列表。列表用\"[\"开始，\"]\"结束。\n",
    "\n",
    "# 要求\n",
    "* 从列表中挑选一个最能总结概括文本内容的值，作为\"标签值\"输出。只能输出列表中的值\n",
    "* 如果无法确定，回复\"其他\"\n",
    "* \"标签值\"字段只能输出列表中的值\n",
    "* 只要输出JSON格式的回复，不要输出其他任何多余的内容\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eee51acb-14b1-4a19-aa65-179e09daa727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(row, attrib):\n",
    "    # ATTRIB_LIST = ['食品类别', '是否为零售食品', '文本主题', '文本类型', '主题相关性'\n",
    "    #               , '情感氛围', '食品用途', '地点场合', '时间场合', '时间', '人物类型'\n",
    "    #               , '痛点', '宏观场合', '微观场合']\n",
    "    \n",
    "    text = row['output']\n",
    "    \n",
    "    # post-process - format\n",
    "    ## remove extra content at the end (去除尾部多余的内容, 且如果没有“}”, 补上)\n",
    "    idx_lst = [index for index, element in enumerate(list(text)) if element == '}']\n",
    "    if len(idx_lst) == 0:\n",
    "        text = text + '}'\n",
    "        idx_lst = [(len(text)-1),]\n",
    "    idx_tail = max(idx_lst)\n",
    "    text = text[:idx_tail+1]\n",
    "    \n",
    "    ## remove extra comma\n",
    "    if text[-2:] == ',}':\n",
    "        text = text[:-2] + '}'\n",
    "    \n",
    "    ## format 原文依据\n",
    "    try:\n",
    "        result = json.loads(text)\n",
    "    except:\n",
    "        # “原文依据” 格式整理, 无 部分统一格式填充\n",
    "        text = re.sub(r'(\"原文依据\": 无[^ ]*)', '\"原文依据\": []', text)\n",
    "        # “原文依据” 格式整理, 确保列表用[]包裹\n",
    "        text = re.sub(r'(\"原文依据\": )((?! \\[)[^}\\n]*)([}\\n])', r'\\1[\\2]\\3', text)\n",
    "\n",
    "    # parse attribute and keywords\n",
    "    result = {}\n",
    "    try:\n",
    "        attr = json.loads(text)\n",
    "        result[attrib] = attr.get('标签值', '未知')\n",
    "        result['关键词-'+attrib] = attr.get('原文依据', [])\n",
    "    except: \n",
    "        result = {attrib: \"PARSING ERROR\", ('关键词-'+attrib): \"PARSING ERROR\"}\n",
    "\n",
    "    return pd.Series(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfb7538a-45e1-4b50-b852-50b4881614fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attribute_keyword(df, attrib):\n",
    "    '''\n",
    "    Return a list/dictionary of pd.DataFrame, each of columns ['arrtibute_type'(标签类别), 'arrtibute_value'(标签名称), 'keyword'(关键词)]\n",
    "    '''\n",
    "    # ATTRIB_LIST = ['食品类别', '是否为零售食品', '文本主题', '文本类型', '主题相关性'\n",
    "    #               , '情感氛围', '食品用途', '地点场合', '时间场合', '时间', '人物类型'\n",
    "    #               , '痛点', '宏观场合', '微观场合']\n",
    "    output_dict = {}\n",
    "    # for c in ATTRIB_LIST:\n",
    "    \n",
    "    df_c = df[~df[attrib].isin(['PARSING ERROR', '未知', ''])][['id', attrib, f'关键词-{attrib}']]\n",
    "    df_c = df_c.explode(f'关键词-{attrib}') \n",
    "    df_c = df_c[~df_c[f'关键词-{attrib}'].isna()]\n",
    "    # remove rows where either tag or keyword is still list\n",
    "    df_c['is_str_tag'] = df_c[attrib].apply(lambda x: 'Y' if isinstance(x, str) else 'N')\n",
    "    df_c['is_str_kwd'] = df_c[f'关键词-{attrib}'].apply(lambda x: 'Y' if isinstance(x, str) else 'N')\n",
    "    df_c = df_c[(df_c['is_str_tag']=='Y')&(df_c['is_str_kwd']=='Y')]\n",
    "    df_c = df_c.drop_duplicates()\n",
    "    df_c['标签类别'] = attrib\n",
    "    output_dict[attrib] = df_c[['id', '标签类别', attrib, f'关键词-{attrib}']]\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b627116c-99e2-46ed-84dc-fa5caed99724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_mysql(df, database, table):\n",
    "    # Create a connection to the database\n",
    "    USER = '<USER>'\n",
    "    PWD = '<PASSWORD>'\n",
    "    URL = '<URL>'\n",
    "    conn = pymysql.connect(host=URL, port=3306, user=USER, passwd=PWD, db=database, charset='utf8mb4')\n",
    "    \n",
    "    rows_fail = []\n",
    "\n",
    "    # Create a cursor\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Iterate over the rows of the DataFrame and insert each one into the database\n",
    "    for i, row in df.iterrows():\n",
    "        # Create an INSERT query\n",
    "        cols = '`'+'`, `'.join(df.columns)+'`'\n",
    "        query = f\"INSERT INTO `{table}` ({cols}) VALUES ({', '.join(['%s'] * len(row))})\"\n",
    "\n",
    "        # Format row data\n",
    "        data = tuple(row[col] for col in df.columns)\n",
    "\n",
    "        try: \n",
    "            # Execute the query\n",
    "            cursor.execute(query, data)\n",
    "\n",
    "            # Commit the changes\n",
    "            conn.commit()\n",
    "        except:\n",
    "            rows_fail.append(data)\n",
    "\n",
    "    # Close the cursor and the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "    return rows_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4327f2-0676-4eeb-b7a8-dadf2c4be56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'validation'\n",
    "attribute_chn = '文本主题'\n",
    "attribute_eng = 'text_topic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47e9b79e-a716-4b08-87b7-80ecb169535f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start index 0\n",
      "Qwen tagging done, took 27.6506609916687 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 100\n",
      "Qwen tagging done, took 29.380916595458984 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 200\n",
      "Qwen tagging done, took 27.326350212097168 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 300\n",
      "Qwen tagging done, took 27.557279109954834 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 400\n",
      "Qwen tagging done, took 28.585430145263672 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 500\n",
      "Qwen tagging done, took 36.28592848777771 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 600\n",
      "Qwen tagging done, took 26.462480545043945 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 700\n",
      "Qwen tagging done, took 25.884853839874268 seconds\n",
      "After normal parsing, 1 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 99 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 800\n",
      "Qwen tagging done, took 26.004806518554688 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 900\n",
      "Qwen tagging done, took 29.316319465637207 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1000\n",
      "Qwen tagging done, took 26.101861238479614 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1100\n",
      "Qwen tagging done, took 30.4343044757843 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1200\n",
      "Qwen tagging done, took 28.17359209060669 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1300\n",
      "Qwen tagging done, took 27.526775121688843 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1400\n",
      "Qwen tagging done, took 29.937382221221924 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1500\n",
      "Qwen tagging done, took 33.709155559539795 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1600\n",
      "Qwen tagging done, took 28.090508222579956 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1700\n",
      "Qwen tagging done, took 30.957513570785522 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1800\n",
      "Qwen tagging done, took 26.765625476837158 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 1900\n",
      "Qwen tagging done, took 28.462390661239624 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2000\n",
      "Qwen tagging done, took 25.690820932388306 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2100\n",
      "Qwen tagging done, took 38.670031785964966 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2200\n",
      "Qwen tagging done, took 29.5717294216156 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2300\n",
      "Qwen tagging done, took 25.906563997268677 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2400\n",
      "Qwen tagging done, took 27.374834775924683 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2500\n",
      "Qwen tagging done, took 28.186227560043335 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2600\n",
      "Qwen tagging done, took 29.124924659729004 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2700\n",
      "Qwen tagging done, took 31.930656671524048 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2800\n",
      "Qwen tagging done, took 29.87130832672119 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n",
      "Start index 2900\n",
      "Qwen tagging done, took 32.85245871543884 seconds\n",
      "After normal parsing, 0 rows have issues\n",
      "Tag keyword dictionary uploaded to mysql\n",
      "Tagged texts uploaded to mysql\n",
      "              , successfully loaded 100 rows to database \"validation\"\n",
      "              , took 0 seconds\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(df_xhs), 100):\n",
    "# for i in range(0, 100, 100):\n",
    "    print(\"Start index\", i)\n",
    "    df = df_xhs.iloc[i:(i+100), :]\n",
    "    \n",
    "    # end\n",
    "    if len(df) == 0:\n",
    "        break\n",
    "    \n",
    "    # tag using LLM\n",
    "    t1 = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL) as executor:\n",
    "        futures = {executor.submit(item_process_tag, row, 'content', MODEL, PROMPT): row for row in df.iterrows()}\n",
    "        result_list = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "    df = pd.DataFrame(result_list).sort_index()\n",
    "    t2 = time.time()\n",
    "    print(f\"Qwen tagging done, took {t2-t1} seconds\")\n",
    "    \n",
    "    # parse LLM output\n",
    "    ## normal case\n",
    "    df_ = df[['output']]\n",
    "    df_ = df_.join(df_.apply(lambda x: parse_output(x, attribute_chn), axis=1))\n",
    "    df = df.merge(df_, on='output', how='left')\n",
    "    \n",
    "    ## need further format\n",
    "    t1 = time.time()\n",
    "    df_error = df[df[attribute_chn]=='PARSING ERROR']\n",
    "    num_error = len(df_error)\n",
    "    print(f\"After normal parsing, {num_error} rows have issues\")\n",
    "    \n",
    "    # extract keyword and attribute accordingly\n",
    "    kwd_dicts = extract_attribute_keyword(df, attribute_chn)\n",
    "    cols_keywords = list(filter(lambda x: x[:3] == '关键词' in x, df_.columns))\n",
    "    df_attrib = df.drop(cols_keywords, axis=1)\n",
    "    df_attrib = df_attrib.fillna('未知')\n",
    "    \n",
    "    # ATTRIB_MAP = {'食品类别':'food_category'\n",
    "    #               , '是否为零售食品':'is_retail_food'\n",
    "    #               , '文本主题':'text_topic'\n",
    "    #               , '文本类型':'text_type'\n",
    "    #               , '主题相关性':'topic_relevance'\n",
    "    #               , '情感氛围':'emotional_atmosphere'\n",
    "    #               , '食品用途':'food_usage'\n",
    "    #               , '地点场合':'where'\n",
    "    #               , '时间场合':'when'\n",
    "    #               , '时间':'time'\n",
    "    #               , '人物类型':'character_type'\n",
    "    #               , '痛点':'painpoint'\n",
    "    #               , '宏观场合':'macro_occasion'\n",
    "    #               , '微观场合':'micro_occasion'}\n",
    "    \n",
    "    # save result\n",
    "    ## save tag-keyword dictionary\n",
    "    for kwd, df_kwd in kwd_dicts.items():\n",
    "        dst_table = f'tag_kwd_{attribute_eng}_prompt_v2'\n",
    "        df_kwd = df_kwd.rename(columns={'id': 'content_id', '标签类别':'tag_category', f'{kwd}':'tag_value', f'关键词-{kwd}':'keyword'})\n",
    "        rows_fail = write_to_mysql(df_kwd, database, dst_table)\n",
    "    print(f'Tag keyword dictionary uploaded to mysql')\n",
    "        \n",
    "    ## save text with attribute \n",
    "    df_attrib = df_attrib.rename(columns={attribute_chn: attribute_eng})\n",
    "    df_attrib_valid = df_attrib[df_attrib[attribute_eng]!='PARSING ERROR']\n",
    "    df_attrib_valid = df_attrib_valid.drop('output', axis=1)\n",
    "\n",
    "    t1 = time.time()\n",
    "    rows_fail_attr = write_to_mysql(df_attrib_valid, database, f'tagged_content_prompt_v2_{attribute_eng}_only')\n",
    "    t2 = time.time()\n",
    "    print(f'''Tagged texts uploaded to mysql\n",
    "              , successfully loaded {len(df_attrib_valid) - len(rows_fail_attr)} rows to database \"{database}\"\n",
    "              , took {int(t2-t1)} seconds''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8336f594-5d14-42ce-876d-3699192ba17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Validation Dataset.xlsx\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52c4874a-0d4d-4ad5-b551-048d227fd320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...</td>\n",
       "      <td>一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...</td>\n",
       "      <td>{'食品类别': '未知', '是否为零售食品': '否', '文本主题': '日常生活挑战...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...   \n",
       "\n",
       "                                             content  \\\n",
       "0  一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...   \n",
       "\n",
       "                                              output  \n",
       "0  {'食品类别': '未知', '是否为零售食品': '否', '文本主题': '日常生活挑战...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f0d6bd8-a5f4-4bad-9e76-746b49b41a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.3 ms, sys: 4.08 ms, total: 44.4 ms\n",
      "Wall time: 74.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SQL_LOAD = '''\n",
    "SELECT `id`, content, text_topic\n",
    "FROM `tagged_content_prompt_v2_text_topic_only`\n",
    "'''\n",
    "\n",
    "df_result = load_data_to_df('validation', SQL_LOAD, columns=['id', 'content', 'text_topic'])\n",
    "            \n",
    "len(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337fa1b5-bd67-40b7-b18a-695f75edd299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['id']].merge(df_result, on='id', how='left')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7849e183-72fe-4992-8bc4-3be52242399f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "      <th>text_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...</td>\n",
       "      <td>一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...</td>\n",
       "      <td>家庭日常</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18336dc645f26e3c3944e8c47c1c9831|d035d1c7e67d6...</td>\n",
       "      <td>津铺子的这个0卡果冻回购很多次了，一共四种口味：白桃、青提、荔枝和芒果。四个口味都还不错，口...</td>\n",
       "      <td>健康与饮食</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b9240d65115b682962187bac31a5a14d|e64571aeb3fbf...</td>\n",
       "      <td>话梅柠檬茶！建议女孩子都把奶茶换成它！ 全网安利的咸柠茶！我能喝一整个夏天！真的好喝到原地转...</td>\n",
       "      <td>健康与饮食</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a0e6ebe573ea14303727123a5abbef3e|de754b5d37397...</td>\n",
       "      <td>随手买的香辣鱼尾也太好吃了吧！鱼肉很有嚼劲，麻麻辣辣的也太过瘾了，平时追剧或者嘴馋了来一包，...</td>\n",
       "      <td>美食分享与推荐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d3fc3728a8c3e13ae5988aa15e4bb0fd|320b8f65d7782...</td>\n",
       "      <td>#踏春赏花最美的景点推荐 #感受大自然的气息和美景 #旅行推荐官 #抖音带你去赏花 #春暖花...</td>\n",
       "      <td>旅行</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id  \\\n",
       "0  e04592c0da33650d9c837464dca6c706|b9393c69e2e6d...   \n",
       "1  18336dc645f26e3c3944e8c47c1c9831|d035d1c7e67d6...   \n",
       "2  b9240d65115b682962187bac31a5a14d|e64571aeb3fbf...   \n",
       "3  a0e6ebe573ea14303727123a5abbef3e|de754b5d37397...   \n",
       "4  d3fc3728a8c3e13ae5988aa15e4bb0fd|320b8f65d7782...   \n",
       "\n",
       "                                             content text_topic  \n",
       "0  一拖二的周六，一早起床，勿勿忙忙准备好小娃需要看病的资料，再准备我需要摆摊的工具，大娃补课催...       家庭日常  \n",
       "1  津铺子的这个0卡果冻回购很多次了，一共四种口味：白桃、青提、荔枝和芒果。四个口味都还不错，口...      健康与饮食  \n",
       "2  话梅柠檬茶！建议女孩子都把奶茶换成它！ 全网安利的咸柠茶！我能喝一整个夏天！真的好喝到原地转...      健康与饮食  \n",
       "3  随手买的香辣鱼尾也太好吃了吧！鱼肉很有嚼劲，麻麻辣辣的也太过瘾了，平时追剧或者嘴馋了来一包，...    美食分享与推荐  \n",
       "4  #踏春赏花最美的景点推荐 #感受大自然的气息和美景 #旅行推荐官 #抖音带你去赏花 #春暖花...         旅行  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "969e3126-2070-4850-80a0-0c3a32445e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"0510_validation_3000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96c55d55-e1df-40a5-8dd6-ddf3a4f0f705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_topic</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>美食分享与推荐</td>\n",
       "      <td>0.304435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>生活感悟</td>\n",
       "      <td>0.063021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>健康与饮食</td>\n",
       "      <td>0.060020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>社交互动</td>\n",
       "      <td>0.044015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>体重管理</td>\n",
       "      <td>0.037346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>广告与推广</td>\n",
       "      <td>0.022007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>家庭日常</td>\n",
       "      <td>0.019340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>健康与养生</td>\n",
       "      <td>0.019340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>节日庆祝</td>\n",
       "      <td>0.018673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>零食推荐</td>\n",
       "      <td>0.016672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>儿童饮食与健康</td>\n",
       "      <td>0.016005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>日常记录</td>\n",
       "      <td>0.016005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>旅行</td>\n",
       "      <td>0.012671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>烹饪与食谱</td>\n",
       "      <td>0.011337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>怀旧与回忆</td>\n",
       "      <td>0.010003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>孕期饮食与健康</td>\n",
       "      <td>0.009336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>日常生活</td>\n",
       "      <td>0.009003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>户外活动</td>\n",
       "      <td>0.009003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>运动健身</td>\n",
       "      <td>0.007669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>文玩核桃</td>\n",
       "      <td>0.007336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_topic  proportion\n",
       "0     美食分享与推荐    0.304435\n",
       "1        生活感悟    0.063021\n",
       "2       健康与饮食    0.060020\n",
       "3        社交互动    0.044015\n",
       "4        体重管理    0.037346\n",
       "5       广告与推广    0.022007\n",
       "6        家庭日常    0.019340\n",
       "7       健康与养生    0.019340\n",
       "8        节日庆祝    0.018673\n",
       "9        零食推荐    0.016672\n",
       "10    儿童饮食与健康    0.016005\n",
       "11       日常记录    0.016005\n",
       "12         旅行    0.012671\n",
       "13      烹饪与食谱    0.011337\n",
       "14      怀旧与回忆    0.010003\n",
       "15    孕期饮食与健康    0.009336\n",
       "16       日常生活    0.009003\n",
       "17       户外活动    0.009003\n",
       "18       运动健身    0.007669\n",
       "19       文玩核桃    0.007336"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df['text_topic'].value_counts(normalize=True).reset_index()).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562c0d2-55c8-459a-b9de-3bbb07f0055a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pepsico]",
   "language": "python",
   "name": "conda-env-pepsico-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
